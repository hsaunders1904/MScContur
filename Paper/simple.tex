\documentclass[floatfix]{article}  
\pagestyle{plain}

\usepackage{amsmath,amssymb}
\usepackage{dsfont}
\usepackage{graphicx} %loads the graphicx.sty package 
\usepackage{epstopdf} %loads the epstopdf.sty package 
\usepackage{slashed}
\usepackage{color}

\usepackage{graphicx,longtable,tocloft,color}
\usepackage{sidecap}
\usepackage{subfig}
\usepackage{xspace}
\usepackage{mydefs}
\usepackage{cite}
\usepackage{lineno}
\usepackage{multirow}
\usepackage{hyperref}
%\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
%\tolerance = 1000
%\parindent 0cm
%\parskip 2mm
%
%
%for draft only:
%\linenumbers
\usepackage{array}

\title{Constraining models of new physics with collider measurements of Standard Model signatures}

\author{Jonathan M. Butterworth$^1$, David Grellscheid$^2$,\\ Michael Kr\"amer$^3$, David Yallup$^1$\\
\it $^1$Department of Physics and Astronomy, University College London,\\ \it Gower St., London, WC1E 6BT, UK\\
\it $^2$IPPP, Department of Physics,\\\it Durham University, DH1 3LE, United Kingdom\\ \it $^3$Institute for Theoretical Particle Physics and Cosmology, \\ \it RWTH Aachen University, Aachen, Germany}


\begin{document}

\maketitle 

\begin{abstract}
The Large Hadron Collider is probing physics in a new kinematic region, at energies around and above the 
electroweak symmetry-breaing scale. Many searches for signatures of physics beyond the Standard Model have 
been performed, and extensive measurements of various final states have also been 
carried out. At time of writing, the results are generally in accord with Standard Model expectations, 
notwithstanding a few currently marginal anomalies. Whether physics beyond the Standard Model is discovered 
or not, there is a need to extract the clearest and most generic information about physics in this new energy regime,
an imperative which will grow with integrated luminosity. The framework of so-called 'simplified models', coupled
with the fact that model-depedence in many LHC measurements is minimal, allows strong constraints on new physics 
models to be placed, using existing and future measurements. We demontrate the power of this approach using a competitive 
simplified Dark Matter model. The method is highly scaleable to other models of further measurements, as they appear.
\end{abstract}


\section{Introduction}
\label{sec:intro}

With the discovery of the Higgs boson~\cite{Aad:2012tfa,Chatrchyan:2012ufa}, 
the first data-taking period of the experiments at the 
Large Hadron Collider demonstrated that the understanding of electroweak symmetry-breaking within
the Standard Model (SM) is broadly correct, and thus that the theory is potentially valid well above the
TeV scale. Many precision measurements of jets, charged leptons, and other of final states, 
have been published, reaching into this new kinematic domain. The predictions of the SM are 
generally in agreement with the data, while the many dedicated searches for physics beyond the SM
have excluded a wide range of possible scenarios. Nevertheless, there are many reasons to be confident that
physics beyond the Standard Model (BSM) exists; examples include the graiational evidence for Dark Matter, the large 
preponderance of matter over antimatter in the universe, and the existence of gravity itself. None of 
these can be easily accommodated within known Standard Model phenomenology. 

This motivates a continued campaign to make precise measurements and calculations at higher energies and 
luminosities, and to exploit these measurements to narrow down the class of viable models of new physics, 
hopefully shedding light on the correct new theory, or at least on the energy scale at which
new physics might be observed at future experiments.

In this paper we exploit three important developments to survey existing measurements and set 
limits on new physics. 

\begin{enumerate}
\item
SM predictions for differential and exclusive, or semi-exclusive, final states are made using sophisticated 
calculational software, often embedded in Monte Carlo generators capable of simulating full, realistic final 
states~\cite{Buckley:2011ms}. These generators now incorporate higher-order processes
matched to logarithmic parton showers and successful models of soft physics such as hadronisation and
underlying event. They are also capable of importing new physics models into this framework, thus allowing
rapid predictions of their impact on a wide variety of final states simultaneously. 
In this paper we make extensive use of these capabilities within Herwig~\cite{Bahr:2008pv}.
\item
As many favoured BSM scenarios have been excluded, there has been a move toward 
``simplified models'' of new physics~\cite{Alves:2011wf,Abercrombie:2015wmb}, which aim to be as generic as possible, 
while being internally consistent. The philosophy is similar to an ``effective lagrangian'' approach in which effective
couplings are introduced to describe new physics, but is more powerful as such simplified models also
include new particles, and thus can remain useful up to and beyond the scale of new physics - a region 
potentially probed by LHC measurements.
\item
The precision measurements from the LHC have mostly been made in a manner which minimises their model-dependence. 
That is, they are defined in terms of final-state signatures in fiducial regions well matched to the
acceptance of the detector. The majority of such measurements are readily available for analysis and
comparison in the Rivet library~\cite{Buckley:2010ar}. 
\end{enumerate}

These three developments together make it possible to efficiently 
bring the power of a very wide range of data to bear on the search for new physics. While such a generic approach is
unlikely to compete in terms of speed and sensitivity with a search optimisied for a specific theory, the breadth
of potential signatures and models which can be covered makes it a powerful complementary approach. Any theory seeking
to explain a new signature or anomaly in the data may have consequences for other final states which should be checked 
against data this way. If no BSM physics emerges, a model-dependent and systematic approach becomes mandatory.

The outline of the paper is as follows. In the next section, we motivate and describe the 
simplified model we consider as a first demonstration, and its implementation via Herwig. 
In Section~\ref{sec:measurements} we introduce the measurements that we will use, and their implementation in Rivet. 
Section~\ref{sec:kinematics} discusses the differential cross sections in which the impact of these models would be 
most apparent. In Section~\ref{sec:limits} this impact is translated into limits on the model parameters.

\section{Simplified Model}\label{sec:models}

... for Dark Matter

Describe the model and its implementation via feynrules and herwig.

\cite{Kahlhoefer:2015bea}

\section{Measurements}\label{sec:measurements}

To be useful in our approach, measurements must be made in as model-independent fashion as possible. 
Cross sections should be measured in a kinematic region closely matching the detector acceptance - commonly called 
'fiducial cross sections' - to avoid extrapolation into unmeasured regions, since such extrapolations must make 
theoretical assumptions; usual that the SM is valid. The measurements should generally be made in terms of observable final
state particles (e.g. leptons, photons) or objects constructed from them (e.g. hadronic jets, missing energy) 
rather than assumed intermediate states ($W, Z, H$, top). Finally, differential measurements are most useful, as features
in the shapes of distributions are a more sensitive test than simple event rates - especially when there are
highly-correlated systematic experimental uncertainties, such as those on the integrated luminosity, or the jet energy scale.

One feature noted in several cases is that missing transverse energy (\MET) is 
explicity assumed to be the same as neutrino transverse energy. In fact, in a BSM study, missing energy can also 
arise from other sources (for example, Dark Matter production!) and so it is important that the result is treated in such a 
way that this sensitivity is correctly estimated. The measurements are typically corrected back to total \MET, 
or to the assumed neutrino $p_T$, in the experimental analysis, using a simulated SM event sample which has been shown to describe
the data well. This involves an extrapolation into the forward 
region where transverse energy is unmeasured; however, unless a BSM particle enters this region, the error made is 
negligible. This means that as long as fiducial acceptance cut is made on BSM particles counting toward \MET (to ensure that large contributions to \MET 
from invisible particles outside the detector acceptance are excluded) such analyses can be 
used\footnote{Of greater consquence, but easier to fix, is the fact that several Rivet methods explicitly calculated \MET
from neutrinos found in the true event record, rather than as the negative of the visible particles in the event. These 
routines were modified as a part of this work, and are all fixed in release 2.5.X.}

The measurements we consider fall into five loose and independent classes.
\begin{enumerate}
\item
Jets: event topologies with any number of jets but no missing energy, leptons, or photons. In this category there
are important measurements from both ATLAS and CMS, many of which have existing Rivet analyses. We make use of 
the highest integrated-luminosity inclusive~\cite{Aad:2014vwa,Chatrchyan:2014gia} dijet~\cite{Aad:2013tea,Aad:2014pua} 
and three-jet~\cite{Aad:2014rma} %{\bf add cms 3jet here? Needs a Rivet routine.} 
measurements made in 7~TeV collisions, as well as the jet mass 
measurement from CMS~\cite{Chatrchyan:2013vbb}.
Unfortunately results from 8~TeV collisions are rarer, and the only one we can use currently is the four-jet 
measurement from ATLAS~\cite{Aad:2015nda}.
\item
Electroweak: events with leptons, with or without missing energy or photons. The high-statistics $W+$jet and $Z+$jet measurements from ATLAS~\cite{Aad:2014qxa,Aad:2013ysa} 
and CMS~\cite{Khachatryan:2014uva}, are used.
We also use the ATLAS $ZZ$ and $W/Z+\gamma$ analyses~\cite{Aad:2012awa,Aad:2013izg}.
\item
Missing energy, possibly with jets but no leptons or photons. There are no fully-correct particle-level distributions available. 
However the ATLAS search for supersymmetry, with jets and missing energy~\cite{Aad:2012fqa} does have a Rivet routine and can be used approximately (I hope)
\item
Isolated photons, with or without missing energy, but no leptons. Here we make use of the inclusive~\cite{Aad:2013zba}, diphoton~\cite{Aad:2012tba} and 
photon-plus-jet~\cite{ATLAS:2012ar} measurements, where available.
\item
Signatures specifically based on top quark or Higgs candidates. These will in principle overlap with
the previous categories depending on decay mode, but we discuss them seperately.
\end{enumerate}

The choice of which measurements to include is driven mainly by the availability of particle-level differential fiducial cross sections 
implemented in Rivet.

\section{Method}

\subsection{Strategy}

The approach taken is to consider simplified BSM models in the light of existing measurements which have already been shown to agree with SM expectations.
Thus this inherently an exercise in limit-setting rather than disovery. The assumption is that a generic, measurement-based approach such as this will not be
competitive in terms of sensitivity, or speed of discovery, with a dediciated search for a specific BSM final-state signature. However, it will have the
advantage of breadth of coverage, and will make a valuable contribution to physics at the energy frontier whether or not new signatures are discovered at the LHC.
In the case of a new discovery, many models will be put forward to explain the data - as has for example already been seen\cite{PhysRevLett.116.150001} 
after the diphoton anomaly 
reported by ATLAS and CMS at the end of 2015 and start of 2016~\cite{ATLAS-CONF-2016-018,CMS-PAS-EXO-16-018}. 
Checking these models for consistency with existing measurements will be vital for unravelling whatever 
the data might be telling us. As will be shown in subsequent sections, models designed to explain on signature may have somewhat unexpected consequences 
in different final states, some of which have already been precisely measured. If it should turn out that no BSM signatures are in the end confirmed at the LHC,
this approach offers potentially the broadest and most generic constraints on new physics, and motivates the most precise possible model-independent measurements
over a wide range of final states, giving the best chance of an indirect pointer to the eventual scale of new physics.

\subsection{Statistical Method}

Following this approach, the question we wish to ask of any given BSM proposal is 'at what significance to existing measurements already exclude this'. 
For all the measurements considered, comparisons to SM calculations have shown consistency between them and the data. Thus as a starting point, we take the
data as our ``null signal'', and we superpose onto them the conttribution from the BSM scenario under consideration. The uncertainties on the data are taken to
be the width of a Gaussian spread, and we use the asymptotic statistical techniques described by Cowan, Cranmer and Gross~\cite{Cowan:2010js} to derive
a liklihood function describing the relative probabilities of the SM and the SM+BSM scenarios given each data point. Since correlations are not well treated by
the Rivet output, we select data to minimise their impact, as described below, and then combine the selected data to obtain an overall exclusion probability
for a given set of BSM paramters from the array of SM measurements considered.
{\bf more detail here?}

\subsection{Dynamical data selection}

Starting with the measurements discussed in Section~\ref{sec:measurements}, we define a prodecure to combine exclusions limits from different measured distributions.
The data used for comparison in Rivet comes in the form of histograms and do not carry information about the correlations between uncertainties - 
even when in several case detailed information is made available in the experimental papers. There are highly correlated uncertainties in several measurements, 
for example on the integrated luminosity, or the energy scale of jet measurements. In some cases these are dominant. Including correlations would be a highly 
complex process even correlations within an individual experimental result were considered, since there are also common systematic uncertainties between different
results, which are generally not provided by the experiments. There are also overlaps between even samples used in many different measurements, which leads to
non-trivial correaltions in the statistical uncertainties. To attempt to avoid spuriously high exclusion rates due to multiply-counting what might be the 
same excursion against several datasets, we take the following approach:
\begin{enumerate}
\item Divide the data into groupings, where there is no overlap between groups in the event samples used in measurements, and hence no statistical correlation 
between groups, but there are some overlaps within each group. These grouping are, crudely, different final states, different experiments, and different beam 
energies.
\item Scan within each group for the most significant deviation between BSM+SM and SM. This is done distribution-by-distribution and bin-by-bin within distributions.
Use only the most significant deviation, and disregard the rest. Although the selection of the most significant devaiation sounds inuitively suspect, in this case it
is a conservative approach, since we are setting limits, and discarding the less-significant bins simply reduces sensitivity. 
The use of a single bin from each measurement removes the dominant effect of highly correlated 
systematic uncertainties within a single measurement. Where a number
of statistically-independent measurements exists {\it within} a group, their liklihoods may be combined to give a single liklihood ratio from the group.
\item Combine the liklihood ratios of the different groups to give a single exclusion limit.
\end{enumerate}


\subsection{Limitations}

We note that our method is best adapted to identify kinematic features (mass peaks, kinematic edges) and will be less sensitive to smooth
deviations in normalisation. In particular, since we take the data to be identically equal to the SM expectation, we will be insentive to any cumulative 
signal which might in principle arise as the cumulative effect of a number of statistically insignificant deviations across a range of experimental measurements.
No such effects are apparent when studying the model considered here, but quantifying this statement is beyond the scope of the current work, and would require 
an extensive evaluation of the theoretical uncertainties on the SM predictions for each channel.




\section{Comparison to Data}\label{sec:kinematics}


\section{Limits}\label{sec:limits}

\section{Conclusions}\label{sec:conclusions}

\section*{Acknowledgments}



\bibliographystyle{h-physrev4}
\bibliography{simple}

\end{document}


