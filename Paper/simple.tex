\documentclass[floatfix]{article}  
\pagestyle{plain}

\usepackage{amsmath,amssymb}
\usepackage{dsfont}
\usepackage{graphicx} %loads the graphicx.sty package 
\usepackage{epstopdf} %loads the epstopdf.sty package 
\usepackage{slashed}
\usepackage{color}

\usepackage{graphicx,longtable,tocloft,color}
\usepackage{sidecap}
\usepackage{subfig}
\usepackage{xspace}
\usepackage{mydefs}
\usepackage{cite}
\usepackage{lineno}
\usepackage{multirow}
\usepackage{hyperref}
%\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
%\tolerance = 1000
%\parindent 0cm
%\parskip 2mm
%
%
%for draft only:
%\linenumbers
\usepackage{array}

\title{Constraining models of new physics with collider measurements of Standard Model signatures}

\author{Jonathan M. Butterworth$^1$, David Grellscheid$^2$,\\ Michael Kr\"amer$^3$, David Yallup$^1$\\
\it $^1$Department of Physics and Astronomy, University College London,\\ \it Gower St., London, WC1E 6BT, UK\\
\it $^2$IPPP, Department of Physics,\\\it Durham University, DH1 3LE, United Kingdom\\ \it $^3$Institute for Theoretical Particle Physics and Cosmology, \\ \it RWTH Aachen University, Aachen, Germany}


\begin{document}

\maketitle 

\begin{abstract}
A new method for providing general consistency contraints Beyond-the-Standard-Model (BSM) theories, using 
measurements at particle colliders, is presented. The method, `Constraints On New Theories Using Rivet' - \Contur \ - exploits
the fact that particle-level differential measurements made in fiducial regions of phase-space have a high degree of
model-independence. These measurements can therefore be compared to BSM physics implemented in Monte Carlo generators in a very
generic way, allowing a wider array of final states to be considered than is typically the case. The \Contur approach should be seen
as complementary to the discovery potential of direct searches, being designed to eliminate inconsistent 
BSM proposals in a context where many (but perhaps not all) measurements are consistent with the Standard Model.
We demontrate, using a competitive simplified Dark Matter model, the power of this approach. 
The \Contur method is highly scaleable to other models and future measurements.
\end{abstract}


\section{Introduction}
\label{sec:intro}

The Large Hadron Collider (LHC) is probing physics in a new kinematic region, at energies around and above the 
electroweak symmetry-breaking scale. With the discovery of the Higgs boson~\cite{Aad:2012tfa,Chatrchyan:2012ufa}, 
the first data-taking period of the LHC experiments demonstrated that the understanding of electroweak symmetry-breaking within
the Standard Model (SM) is broadly correct, and thus that the theory is potentially valid well above the
TeV scale. Many precision measurements of jets, charged leptons, and other of final states, 
have been published, reaching into this new kinematic domain. The predictions of the SM are 
generally in agreement with the data, while the many dedicated searches for physics beyond the SM
have excluded a wide range of possible scenarios. Nevertheless, there are many reasons to be confident that
physics beyond the Standard Model (BSM) exists; examples include the graiational evidence for Dark Matter, the large 
preponderance of matter over antimatter in the universe, and the existence of gravity itself. None of 
these can be easily accommodated within known Standard Model phenomenology. 

This motivates a continued campaign to make precise measurements and calculations at higher energies and 
luminosities, and to exploit these measurements to narrow down the class of viable models of new physics, 
hopefully shedding light on the correct new theory, or at least on the energy scale at which
new physics might be observed at future experiments. Whether physics beyond the Standard Model is discovered 
or not, there is a need to extract the clearest and most generic information about physics in this new energy regime,
an imperative which will grow with integrated luminosity. 

In this paper we exploit three important developments to survey existing measurements and set 
limits on new physics. 

\begin{enumerate}
\item
SM predictions for differential and exclusive, or semi-exclusive, final states are made using sophisticated 
calculational software, often embedded in Monte Carlo generators capable of simulating full, realistic final 
states~\cite{Buckley:2011ms}. These generators now incorporate matrix-elements for higher-order processes
matched to logarithmic parton showers, and successful models of soft physics such as hadronisation and
the underlying event. They are also capable of importing new physics models into this framework, thus allowing
the rapid prediction of their impact on a wide variety of final states simultaneously. 
In this paper we make extensive use of these capabilities within Herwig~\cite{Bahr:2008pv}.
\item
As many favoured BSM scenarios have been excluded, there has been a move toward 
``simplified models'' of new physics~\cite{Alves:2011wf,Abercrombie:2015wmb}, which aim to be as generic as possible, 
while being internally consistent. The philosophy is similar to an ``effective lagrangian'' approach in which effective anomalous
couplings are introduced to describe new physics, but is more powerful as such simplified models also
include new particles, and thus can remain useful up to and beyond the scale of new physics - a region 
potentially probed by LHC measurements.
\item
The precision measurements from the LHC have mostly been made in a manner which minimises their model-dependence. 
That is, they are defined in terms of final-state signatures in fiducial regions well matched to the
acceptance of the detector. Many such measurements are readily available for analysis and
comparison in the Rivet library~\cite{Buckley:2010ar}. 
\end{enumerate}

These three developments together make it possible to efficiently 
bring the power of a very wide range of data to bear on the search for new physics. While such a generic approach is
unlikely to compete in terms of speed and sensitivity with a search optimisied for a specific theory, the breadth
of potential signatures and models which can be covered makes it a powerful complementary approach. Any theory seeking
to explain a new signature or anomaly in the data may have consequences for other final states which should be checked 
against data this way. If no BSM physics emerges, a model-dependent and systematic approach becomes mandatory.

The outline of the paper is as follows. In the next section, we motivate and describe the 
simplified model we consider as a first demonstration, and its implementation via Herwig. 
In Section~\ref{sec:measurements} we introduce the measurements that we will use, and their implementation in Rivet. 
Section~\ref{sec:kinematics} discusses the differential cross sections in which the impact of these models would be 
most apparent. In Section~\ref{sec:limits} this impact is translated into limits on the model parameters.

\section{Simplified Model}\label{sec:models}

... for Dark Matter

Describe the model and its implementation via feynrules and herwig.

\cite{Kahlhoefer:2015bea}

\section{Measurements}\label{sec:measurements}

To be useful in our approach, measurements must be made in as model-independent fashion as possible. 
Cross sections should be measured in a kinematic region closely matching the detector acceptance - commonly called 
'fiducial cross sections' - to avoid extrapolation into unmeasured regions, since such extrapolations must make 
theoretical assumptions; usual that the SM is valid. The measurements should generally be made in terms of observable final
state particles (e.g. leptons, photons) or objects constructed from them (e.g. hadronic jets, missing energy) 
rather than assumed intermediate states ($W, Z, H$, top). Finally, differential measurements are most useful, as features
in the shapes of distributions are a more sensitive test than simple event rates - especially when there are
highly-correlated systematic experimental uncertainties, such as those on the integrated luminosity, or the jet energy scale.

One feature noted in several cases is that missing transverse energy (\MET) is 
explicity assumed to be the same as neutrino transverse energy. In fact, in a BSM study, missing energy can also 
arise from other sources (for example, Dark Matter production!) and so it is important that the result is treated in such a 
way that this sensitivity is correctly estimated. The measurements are typically corrected back to total \MET, 
or to the assumed neutrino $p_T$, in the experimental analysis, using a simulated SM event sample which has been shown to describe
the data well. This involves an extrapolation into the forward 
region where transverse energy is unmeasured; however, unless a BSM particle enters this region, the error made is 
negligible. This means that as long as fiducial acceptance cut is made on BSM particles counting toward \MET (to ensure that 
large contributions to \MET 
from invisible particles outside the detector acceptance are excluded) such analyses can be 
used\footnote{Of greater consquence, but easier to fix, is the fact that several Rivet methods explicitly calculated \MET
from neutrinos found in the true event record, rather than as the negative of the visible particles in the event. These 
routines were modified as a part of this work, and are all fixed in release 2.5.X.}

The measurements we consider fall into five loose and independent classes.
\begin{enumerate}
\item
Jets: event topologies with any number of jets but no missing energy, leptons, or photons. In this category there
are important measurements from both ATLAS and CMS, many of which have existing Rivet analyses. We make use of 
the highest integrated-luminosity inclusive~\cite{Aad:2014vwa,Chatrchyan:2014gia} dijet~\cite{Aad:2013tea,Aad:2014pua} 
and three-jet~\cite{Aad:2014rma} %{\bf add cms 3jet here? Needs a Rivet routine.} 
measurements made in 7~TeV collisions, as well as the jet mass 
measurement from CMS~\cite{Chatrchyan:2013vbb}.
Unfortunately results from 8~TeV collisions are rarer, and the only one we can use currently is the four-jet 
measurement from ATLAS~\cite{Aad:2015nda}.
\item
Electroweak: events with leptons, with or without missing energy or photons. The high-statistics $W+$jet and $Z+$jet measurements from ATLAS~\cite{Aad:2014qxa,Aad:2013ysa} 
and CMS~\cite{Khachatryan:2014uva}, are used.
We also use the ATLAS $ZZ$ and $W/Z+\gamma$ analyses~\cite{Aad:2012awa,Aad:2013izg}.
\item
Missing energy, possibly with jets but no leptons or photons. There are no fully-correct particle-level distributions available. 
However the ATLAS search for supersymmetry, with jets and missing energy~\cite{Aad:2012fqa} does have a Rivet routine and can be used approximately (I hope)
\item
Isolated photons, with or without missing energy, but no leptons. Here we make use of the inclusive~\cite{Aad:2013zba}, diphoton~\cite{Aad:2012tba} and 
photon-plus-jet~\cite{ATLAS:2012ar} measurements, where available.
\item
Signatures specifically based on top quark or Higgs candidates. Most such measurements to date have been made at the 'parton' level (that is,
corrected using SM MC back to the top or Higgs before decay, and many of them are extrapolated to $4\pi$ phase space. Both steps increase
the model dependence and make them unsuitable for the \Contur approach. Recently, however, fiducial, differential, particle-level measurements
have begun to appear\cite{}. These could are potentially very powerful in excluding some models, but will in principle overlap with the previous 
categories depending on decay mode. We leave the inclusion of such measurements for future work.
\end{enumerate}

The choice of which measurements are actually included at this stage is driven mainly by the availability of particle-level differential 
fiducial cross sections 
implemented in Rivet.

\section{Method}

\subsection{Strategy}

The approach taken is to consider simplified BSM models in the light of existing measurements which have already been shown to agree with SM expectations.
Thus this is inherently an exercise in limit-setting rather than disovery. The assumption is that a generic, measurement-based approach such as this will not be
competitive in terms of sensitivity, or speed of discovery, with a dediciated search for a specific BSM final-state signature. However, it will have the
advantage of breadth of coverage, and will make a valuable contribution to physics at the energy frontier whether or not new signatures are discovered at the LHC.
In the case of a new discovery, many models will be put forward to explain the data - as has for example already been seen\cite{PhysRevLett.116.150001} 
after the 750~GeV diphoton anomaly 
reported by ATLAS and CMS at the end of 2015 and start of 2016~\cite{ATLAS-CONF-2016-018,CMS-PAS-EXO-16-018}. 
Checking these models for consistency with existing measurements will be vital for unravelling whatever 
the data might be telling us. As will be shown in subsequent sections, models designed to explain one signature may have somewhat unexpected consequences 
in different final states, some of which have already been precisely measured. If it should turn out that no BSM signatures are in the end confirmed at the LHC,
\Contur offers potentially the broadest and most generic constraints on new physics, and motivates the most precise possible model-independent measurements
over a wide range of final states, giving the best chance of an indirect pointer to the eventual scale of new physics.

\subsection{Dynamical data selection}

Starting with the measurements discussed in Section~\ref{sec:measurements}, we define a prodecure to combine exclusions limits from different measured distributions.
The data used for comparison in Rivet come in the form of histograms, which do not carry information about the correlations between uncertainties - 
even when in several case detailed information is made available in the experimental papers. There are highly correlated uncertainties in several measurements, 
for example on the integrated luminosity, or the energy scale of jet measurements. In some cases these are dominant. Including correlations would be a highly 
complex process, since as well as correlations within a single data-set, there are also common systematic uncertainties between different
results, which are generally not provided by the experiments. There are also overlaps between event samples used in many different measurements, which lead to
non-trivial correlations in the statistical uncertainties. To attempt to avoid spuriously high exclusion rates due to multiply-counting what might be the 
same excursion against several datasets, we take the following approach:
\begin{enumerate}
\item Divide the data into groupings, where there is no overlap between groups in the event samples used in measurements, and hence no statistical correlation 
between groups, but there are some overlaps within each group. These grouping are, crudely, different final states, different experiments, and different beam 
energies.
\item Scan within each group for the most significant deviation between BSM+SM and SM. This is done distribution-by-distribution and bin-by-bin within distributions.
Use only the most significant deviation, and disregard the rest. Although the selection of the most significant deviation sounds inuitively suspect, in this case it
is a conservative approach, since we are setting limits, and discarding the less-significant bins simply reduces sensitivity. 
The use of a single bin from each measurement removes the dominant effect of highly correlated 
systematic uncertainties within a single measurement. Where a number
of statistically-independent measurements exists {\it within} a group, their likelihoods may be combined to give a single likelihood ratio from the group.
\item Combine the likelihood ratios of the different groups to give a single exclusion limit.
\end{enumerate}

\subsection{Statistical Method}

The question we wish to ask of any given BSM proposal is {\it `at what significance do existing measurements already exclude this'}. 
For all the measurements considered, comparisons to SM calculations have shown consistency between them and the data. Thus as a starting point, we take the
data as our ``null signal'', and we superpose onto them the contribution from the BSM scenario under consideration.
%The uncertainties on the data are taken to
%be the width of a Gaussian spread, and we use the asymptotic statistical techniques described by Cowan, Cranmer and Gross~\cite{Cowan:2010js} to derive
%a liklihood function describing the relative probabilities of the SM and the SM+BSM scenarios given each data point. 

Taking each bin of each distribution considered as a separate statistic to be tested, a likelihood function can be constructed as follows,
\begin{align}
L(\mu, \vec{b}, \vec{\sigma}_{b}) = { \frac{(\mu s + b)^{n}}{n!} \exp\left(-\mu s + b\right) \times \frac{1}{\sqrt{2 \pi} \sigma_{b}} \exp\left(-\frac{(\tilde{b} - b)^{2}}{2 \sigma_{b}^{2}}\right)}
\end{align}
Where the following components are identified:
\begin{itemize}
\item A Poisson event count, noting that the measurements considered are differential cross section measurements, hence the counts are multiplied by a factor of the integrated luminosity taken from the experimental paper behind each analysis, to convert to an event count in each bin (and subsequently the additional events that the new physics would have added to the measurement made). This statistic in each tested bin then is comprised of:
\begin{itemize}
\item $s$, the BSM signal event count
\item $b$, the background count, taken as the central value of the data points
\item $n$, the total count taken as $s+b$
\item $\mu$, the signal strength parameter modulating the strength of the signal hypothesis tested, thus $\mu=0$ corresponds to the null hypothesis and $\mu=1$ the full signal strength hypothesis
\end{itemize}
\item a convolution with a Gaussian encoding the error on the background count, $b$, where the following additional components are identified:
\begin{itemize}
\item $\tilde{b}$, a variable background count, at face value this is equal to $b$, however it is marked differently as it can be varied in order to utilise Monte Carlo methods to extract the sensitivity if desired.
\item $\sigma_{b}$, Uncertainty in $b$ count taken from Rivet as 1 $\sigma$ error on a Gaussian (uncertainties taken as the combination of statistical and systematics uncertaintes in quadrature. Typically the systematic uncertainty dominates).
\end{itemize}
\end{itemize}

This likelihood function is then used to construct a profile likelihood ratio. The formalism and methods followed are taken from 
Ref.~\cite{Cowan:2010js}, in particular the test statistic, $\tilde{q}_{\mu}$ is constructed. This enables the setting of a one-sided upper limit on the confidence 
in the strength parameter hypothesis, $\mu$, desirable since in the situation that the observed strength parameter exceeds the tested hypothesis, agreement with 
the hypothesis should not diminish. In addition this construction places a lower limit on the strength parameter, where any observed fluctuations below the null 
hypothesis are said to agree with the null hypothesis. In \Contur, the latter point will be unimportant, as the manner in which samples are 
generated and tested will only increase the event rates w.r.t the null hypothesis. 
\\

A particular result adopted from Ref.~\cite{Cowan:2010js} uses the so called Asimov data set to build an approximate distribution of the considered test statistic. 
As a cross-check, the sampling distribution for the test statistic was also estimated via Monte Carlo methods for multiple individual tests of various channels, 
in all cases the result from Cowan et al. is observed, that there is a high level of agreement between the values obtained via the two methods, hence it can be said
that the tested parameter space values fall in the asymptotic, or large sample, limit of the distributions checked~\footnote{~This is not unexpected, the
construction up to this point has been designed to look at smoothly falling well-measured processes at energies that the LHC is designed to probe. This is however a 
result that should be monitored when considering different models}. Specifically, the Asimov data set is used to extract an approximation for the variance and central value of the strength parameter, $\mu$, and the results of Wilks~\cite{Wilks:1938dza} and Wald~\cite{Wald:1943:EWM} (again as outlined in Ref.~\cite{Cowan:2010js}) are used to approximate the test statistic. 
Hence the $p$-value quantifying the agreement between the null (SM only) hypothesis and the tested hypothesis can be computed. 
\\

In fact, as is convention in the particle physics community, the final measure of statistical agreement is presented in terms of what is known as the CL$_{s}$ 
method~\cite{Junk:1999kv,Read:2002hq}. This is simply a matter of convention, since as there is exact agreement in this method between the central value of the 
data and the null hypothesis, the CL$_{s}$ value differs only slightly from calculating the CL of the $s+b$ hypothesis. Then, for a given distribution, CL$_{s}$ 
can be evaluated separately for each bin, where the bin with the largest CL$_{s}$ value (and correspondingly smallest $p_{s+b}$ value) is taken to represent the 
sensitivity measure utilised to evaluate each distribution, a process outlined in section~\ref{subsec:selec}. Armed then with a list of selected sensitive 
distributions with minimal correlations, a total combined CL$_{s}$ across all considered channels can then be constructed. This is a trivial extension of 
the methodology presented in this section, and is simply taken as a product of all the selected counts in a new combined likelihood function. 
The rest of the process is unchanged when considering the combination


\subsection{Limitations}

We note that our method is best adapted to identify kinematic features (mass peaks, kinematic edges) and will be less sensitive to smooth
deviations in normalisation. In particular, since we take the data to be identically equal to the SM expectation, we will be insentive 
to any cumulative signal which might in principle arise as the cumulative effect of a number of statistically insignificant deviations 
across a range of experimental measurements.
No such effects are apparent when studying the model considered here, but quantifying this statement is beyond the scope of the current work, 
and would require an extensive evaluation of the theoretical uncertainties on the SM predictions for each channel.




\section{Comparison to Data}\label{sec:kinematics}


\section{Limits}\label{sec:limits}

\section{Conclusions}\label{sec:conclusions}

\section*{Acknowledgments}



\bibliographystyle{h-physrev4}
\bibliography{simple}

\end{document}


